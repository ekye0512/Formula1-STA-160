# Predicting Formula 1 Finishing Positions with Machine Learning  
**Final Capstone Project — STA 160 @ UC Davis**

Hi! I’m **Eric Kye**, a huge Formula 1 fan and a stats major at UC Davis.  
This project is my final capstone for **STA 160** and wraps up my four-year undergrad journey in statistical data science.

Using historical race data from a Kaggle competition, I applied machine learning models to predict driver finishing positions (1–20). This challenge let me bring together everything I’ve learned—data cleaning, visualization, model building—and apply it to a real-world problem in a sport I love.

---

## Project Overview
- **Course**: STA 160 - Statistical Learning
- **Dataset**: Formula 1 race data (2.8M+ rows cleaned to 21,998)
- **Goal**: Predict race finishing positions using pre-race features
- **Models Used**:
  - Random Forest (Best - RMSE: 4.47)
  - XGBoost
  - Gradient Boosting
  - Lasso Regression
  - Ridge Regression

---

## Key Takeaways
- Tree-based models worked best with F1’s complex, non-linear dynamics
- Starting grid, wins, and season points were the most predictive features
- XGBoost improved with tuning, but Random Forest remained the most accurate
- Data cleaning was the most important (and hardest) step

---

## Files
- `sta160project.ipynb`  
  Full Jupyter notebook with data processing, EDA, modeling, evaluation, and reflections.

---

## Reflections
This wasn’t just a class project. It was a chance to connect my passion for F1 with everything I’ve been learning in class. It reminded me how messy real data can be—and how rewarding it is when you get it right.

---

## Tools Used
- Python (pandas, scikit-learn, xgboost, matplotlib, seaborn)
- Jupyter Notebook
- Kaggle Formula 1 Datathon dataset

---
